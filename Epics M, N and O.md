
ğŸ•¯ï¸ Epic M â€” Decommission & Resurrection Protocols (Blueprint ExecutÃ¡vel)
=========================================================================

> Objetivo: desligar a VVTV **com dignidade computÃ¡vel**, hibernar **sem risco**, e **ressuscitar** com integridade total â€” incluindo trilha forense, assinatura, e â€œframe finalâ€ como prova.

**Escopo do Epic**

*   **M1 â€” Ritual de Desligamento ComputÃ¡vel**
*   **M2 â€” HibernaÃ§Ã£o & GuardiÃ£o**
*   **M3 â€” RessurreiÃ§Ã£o/Pick-up Total**
*   SeguranÃ§a, evidÃªncias, testes e runbooks operacionais

* * *

ğŸ“¦ EntregÃ¡veis (artefatos gerados)
----------------------------------

*   `final_frame.jpg` + `final_frame.json` (metadados tÃ©cnicos + assinatura)
*   `last_manifest.json` (hashes, tamanhos, versÃµes, seeds)
*   Snapshots compactados:
    *   `snapshot_data.tar.zst` (storage, HLS, mezzanines, somente leitura)
    *   `snapshot_db.tar.zst` (SQLite \*.sqlite + WAL + schema)
    *   `snapshot_system.tar.zst` (configs, scripts, plists, versÃµes)
*   `resurrection_playbook.md` (passo a passo verificado)
*   `signature.sig` (Ed25519 do conjunto)
*   `resurrection_report.json` (relato pÃ³s-revival)

* * *

ğŸ§­ Estado e diretÃ³rios padrÃ£o
-----------------------------

```
/vvtv/
â”œâ”€ broadcast/          # HLS atual, nginx-rtmp
â”œâ”€ storage/            # mezzanine e prontos
â”œâ”€ data/               # *.sqlite (plans, queue, metrics, economyâ€¦)
â”œâ”€ system/             # bin/, logs/, configs toml
â”œâ”€ vault/              # keys/, manifests/, snapshots/
â””â”€ monitor/            # dashboard, captures
```

* * *

âœ… M1 â€” Ritual de Desligamento ComputÃ¡vel
----------------------------------------

### Objetivo

Congelar o sistema **em estado Ã­ntegro**, com **prova criptogrÃ¡fica** e **manifesto verificÃ¡vel**.

### Entregas

*   Script: `/vvtv/system/bin/m1_shutdown_ritual.sh`
*   Manifesto: `/vvtv/vault/manifests/last_manifest.json`
*   Snapshot: `/vvtv/vault/snapshots/<YYYY-mm-dd_HHMM>/*`

### Fluxo (passo a passo)

1.  **PrÃ©-check (hard stop se falhar)**
    *   Stream ativo mas **sem itens crÃ­ticos rodando**:
    *   `queue_length >= 1h` (buffer garantido) **OU** `paused=true`
    *   FFmpeg estÃ¡vel > 60s (sem quedas)
*   Bancos Ã­ntegros:
    *   `PRAGMA integrity_check` em todos os \*.sqlite (fail â†’ abort)
*   Disco, temp, CPU:
    *   `disk_free > 15%`, `temp_cpu < 80Â°C`
        2.  **Congelamento do agendador**
    *   `planner: pause`, `realizer: pause`, `discovery: pause`
*   Zerar download/transcode pendentes (aguardar atÃ© `in_progress=0`)
*   Setar `queue.freeze=true` (sem reordenaÃ§Ãµes automÃ¡ticas)
    3.  **Frame final + Telemetria**
    *   Capturar `final_frame.jpg` do stream pÃºblico (curl+ffmpeg)
*   Extrair:
    *   `vmaf_live`, `stream_bitrate`, `segment_age`, `viewer_count?`
*   Criar `final_frame.json`:
    *   `ts_utc`, `slot_id`, `plan_id_executado`, `ffmpeg_args`, KPIs
        4.  **Manifesto & Assinatura**
    *   Calcular SHA-256 recursivo:
    *   `/broadcast`, `/storage/ready`, `/system/configs`, `/data/*.sqlite`
*   Montar `last_manifest.json`:
    *   versÃµes (Rust, FFmpeg, nginx), seeds de seleÃ§Ã£o, business\_logic hash
*   Assinar (Ed25519) com chave do **Fundador** (`/vvtv/vault/keys/foundation_ed25519`)
    *   `signature.sig` anexado ao manifesto
        5.  **Snapshots**
    *   `snapshot_db.tar.zst` (inclui \*.sqlite + WAL + schema.sql)
*   `snapshot_system.tar.zst` (TOML, plists, bin, versÃµes, commit ids)
*   `snapshot_data.tar.zst` (somente **mezzanines/HLS**; excluir cache/tmp)
*   Arquivar em `/vvtv/vault/snapshots/<stamp>/`
    6.  **Encerramento â€œlimpoâ€**
    *   `broadcaster: stop ingest` (gravar Ãºltimo segmento)
*   `nginx-rtmp: stop` (graceful)
*   **Opcional**: `closing_bell.mp4` (5s) como baixa simbÃ³lica
*   Emitir `Last Transmission Manifest` em markdown
    *   â€œÃšltimo slot, plano, duraÃ§Ã£o executada, mÃ©tricas finaisâ€¦â€

### Script (nÃºcleo simplificado)

```bash
#!/usr/bin/env bash
set -euo pipefail
STAMP="$(date -u +'%Y-%m-%d_%H%MZ')"
BASE="/vvtv"; VAULT="$BASE/vault"; SNAP="$VAULT/snapshots/$STAMP"
mkdir -p "$SNAP"

echo "[M1] PrÃ©-checksâ€¦"
"$BASE/system/bin/selfcheck.sh" --strict

echo "[M1] Congelando agendadoresâ€¦"
vvtvctl planner pause
vvtvctl realizer pause
vvtvctl discovery pause
vvtvctl queue freeze --on

echo "[M1] Aguardando pipelines concluÃ­remâ€¦"
vvtvctl status --wait-empty --timeout 600

echo "[M1] Capturando frame finalâ€¦"
ffmpeg -y -i "http://127.0.0.1/hls/main.m3u8" -frames:v 1 "$SNAP/final_frame.jpg"
"$BASE/system/bin/capture_telemetry.sh" > "$SNAP/final_frame.json"

echo "[M1] Calculando manifestoâ€¦"
"$BASE/system/bin/make_last_manifest.sh" > "$SNAP/last_manifest.json"
logline sign --key "$VAULT/keys/foundation_ed25519" --in "$SNAP/last_manifest.json" --out "$SNAP/signature.sig"

echo "[M1] Gerando snapshotsâ€¦"
tar -I 'zstd -19' -cf "$SNAP/snapshot_db.tar.zst" -C "$BASE/data" .
tar -I 'zstd -19' -cf "$SNAP/snapshot_system.tar.zst" -C "$BASE/system" .
tar -I 'zstd -19' -cf "$SNAP/snapshot_data.tar.zst" -C "$BASE" storage broadcast

echo "[M1] Parando broadcastâ€¦"
vvtvctl broadcaster stop
sudo systemctl stop nginx-rtmp || true

echo "[M1] Last Transmission pronta em $SNAP"
```

### CritÃ©rios de aceitaÃ§Ã£o

*   Snapshot verificÃ¡vel (hashes conferem) + assinatura vÃ¡lida
*   `last_manifest.json` completo (versÃµes, seeds, hashes, tamanhos)
*   Nenhuma escrita pendente nos bancos (WAL vazia)
*   â€œFrame finalâ€ reproduzÃ­vel

* * *

ğŸ’¤ M2 â€” HibernaÃ§Ã£o & GuardiÃ£o
-----------------------------

### Objetivo

Manter o **estado estÃ¡tico e Ã­ntegro** atÃ© ordem de retorno, com **vigilÃ¢ncia mÃ­nima**.

### Entregas

*   Daemon: `sleepguardd` (Rust/tokio, ciclo 15 min)
*   `resume.sh` e `lockdown.sh` (shell)

### PolÃ­ticas

*   **Filesystem**: tornar **somente-leitura** tudo que for snapshotado
*   **ServiÃ§os**: _down_, watchdogs parados, timers desativados
*   **Chaves**: rotacionar Tailscale/chaves que viriam a expirar em 60d
*   **Monitoramento**: `sleepguardd` verifica:
    *   integridade (hashes do snapshot vs atual)
    *   modificaÃ§Ãµes indevidas (touch/mtime)
    *   tentativa de gravaÃ§Ã£o â†’ alerta

### Fluxo

1.  **Lockdown**
    *   Remontar dirs crÃ­ticos como ro (`chflags uchg` no macOS, `chattr +i` no Linux onde possÃ­vel)
*   Desligar timers/launchd/systemd dos mÃ³dulos
*   Ativar `sleepguardd` (somente leitura, logs locais securizados)
    2.  **GuardiÃ£o**
    *   A cada 15 min:
    *   `sha256sum` de amostra (amostragem 1:N arquivos por rodada)
    *   Verifica permissÃµes e flags de imutabilidade
    *   Checa espaÃ§o (disco nÃ£o pode encher)
    *   Emite `sleepguard_report.jsonl` (append-only)
        3.  **Retorno (resume)**
    *   Inverter flags **com checklist**
*   Recarregar serviÃ§os em ordem segura:
    1.  `nginx-rtmp` (sem ingest)
    2.  bancos SQLITE (modo RW)
    3.  planner/realizer (descongelar)
    4.  broadcaster (voltar ingest)

### Scripts essenciais

```bash
# lockdown.sh
sudo chattr +i /vvtv/vault/snapshots/* || true
sudo launchctl unload -w â€¦/planner.plist || true
sudo launchctl unload -w â€¦/watchdog.plist || true
echo '{"state":"hibernating","ts":"'$(date -u +%FT%TZ)'"}' >> /vvtv/vault/sleepguard_state.jsonl

# resume.sh
sudo chattr -i /vvtv/vault/snapshots/* || true
sudo launchctl load -w â€¦/nginx-rtmp.plist
sudo launchctl load -w â€¦/db-maint.plist
vvtvctl planner resume
vvtvctl realizer resume
vvtvctl discovery resume
vvtvctl queue freeze --off
vvtvctl broadcaster start
```

### CritÃ©rios de aceitaÃ§Ã£o

*   `sleepguardd` rodando, relatÃ³rios sem divergÃªncias por 24h
*   Flags de imutabilidade ativos em snapshots/manifestos
*   `resume.sh` repÃµe os serviÃ§os sem erro e sem perda de estado

* * *

ğŸ” M3 â€” Protocolo de RessurreiÃ§Ã£o Completo
------------------------------------------

### Objetivo

Subir **em mÃ¡quina nova** (ou nÃ³ backup) **partindo do snapshot**, validando integridade e provando continuidade (frame final = frame inicial do revival).

### Entregas

*   `logline revive` (wrapper/CLI)
*   `resurrection_playbook.md` e `resurrection_report.json`

### Fluxo em nÃ³ de revival

1.  **Provisionar base**
    *   Instalar dependÃªncias mÃ­nimas (FFmpeg, nginx-rtmp, Rust runtime)
*   Criar Ã¡rvore `/vvtv/â€¦`
*   Copiar pacote `snapshot_*.tar.zst` + `last_manifest.json` + `signature.sig`
    2.  **VerificaÃ§Ã£o**
    *   `logline verify`:
    *   valida assinatura (Ed25519)
    *   confere hashes no `last_manifest.json`
*   **Fail** â†’ aborta revival (gera relatÃ³rio e para)
    3.  **RestauraÃ§Ã£o**
    *   Descompactar `snapshot_system` â†’ `/vvtv/system`
*   Descompactar `snapshot_db` â†’ `/vvtv/data`
*   Descompactar `snapshot_data` â†’ `/vvtv/{storage,broadcast}`
*   Rodar migraÃ§Ãµes se necessÃ¡rio (`schema.sql`)
    4.  **Sanidade**
    *   `PRAGMA integrity_check` em todos os bancos
*   Conferir versÃµes do binÃ¡rio vs manifesto (alertar se divergirem)
*   Subir `nginx-rtmp` **sem ingest** e testar `/status`
    5.  **Prova de continuidade**
    *   Iniciar `broadcaster` com o primeiro item que estava â€œa executarâ€ no desligamento
*   Capturar `first_frame_after_revive.jpg` â€” **deve ser igual** (ou tolerÃ¢ncia perceptual) ao `final_frame.jpg`
    *   Caso seja HLS live, validaâ€se por fingerprint + metadados de playout
        6.  **ReativaÃ§Ã£o**
    *   `planner/realizer` retomam com `queue.freeze=false`
*   `discovery` continua **congelado** por 24h (recomposiÃ§Ã£o do buffer)
*   Watchdogs reabilitados gradualmente (encoders â†’ nginx â†’ fila)
    7.  **Relato**
    *   Salvar `resurrection_report.json` (tempos, divergÃªncias, correÃ§Ãµes)
*   Atualizar `vault/history/` com o Generation+1 + `ancestor_signature`

### CLI (pseudo)

```bash
logline revive \
  --snapshots /mnt/snap/2025-10-21_2130Z \
  --verify /mnt/snap/last_manifest.json:/mnt/snap/signature.sig \
  --target /vvtv \
  --strict \
  --prove-continuity
```

### CritÃ©rios de aceitaÃ§Ã£o

*   Assinatura **vÃ¡lida** e hashes **iguais**
*   Bancos Ã­ntegros e serviÃ§os sobem **sem warnings crÃ­ticos**
*   Prova de continuidade **aprovada** (frame/manifesto)
*   RelatÃ³rio final gerado e arquivado

* * *

ğŸ” SeguranÃ§a & Forense (todas as fases)
---------------------------------------

*   **Assinatura Ed25519** do manifesto e dos relatÃ³rios (cadeia de custÃ³dia)
*   `chattr +i`/`uchg` e permissÃµes 600/700 nas chaves/relatÃ³rios
*   Logs **append-only**: `sleepguard_report.jsonl`, `m1_artifacts.jsonl`, `resurrection_report.jsonl`
*   **Air-gap opcional**: exportar snapshots para mÃ­dia offline (B2/NAS/Cold Storage)
*   Detector de DRM/CSAM permanece **ativo** e impede revival com conteÃºdo problemÃ¡tico

* * *

ğŸ§ª Testes e Ensaios
-------------------

*   **Dry-run mensal** do M1 em sandbox:
    *   Gere um snapshot pequeno (amostra de dados) e valide assinatura
*   **Revival trimestral** em nÃ³ reserva:
    *   Medir RTO (objetivo: < 30 min) e RPO (objetivo: 0, Ã­ntegro)
*   **Chaos drills**:
    *   CorrupÃ§Ã£o simulada de 1 arquivo â†’ revival deve **falhar** com relatÃ³rio claro
    *   DivergÃªncia de versÃ£o de FFmpeg â†’ **warning** e autoâ€ajuste do preset

* * *

ğŸ“‹ Runbooks
-----------

### Operador â€” Desligar hoje (M1)

1.  `vvtvctl status` â†’ conferir filas vazias ou freeze
2.  `m1_shutdown_ritual.sh`
3.  Conferir `â€¦/snapshots/<stamp>/*` + `signature.sig`
4.  `lockdown.sh` (se for hibernar)

### Operador â€” Retornar amanhÃ£ (M2)

1.  `resume.sh`
2.  `vvtvctl broadcaster start` â†’ smoke test
3.  `planner/realizer/discovery` â†’ retomar em sequÃªncia
4.  Monitorar buffer > 1h

### Operador â€” CatÃ¡strofe, novo nÃ³ (M3)

1.  Restaurar snapshots + `logline revive --strict`
2.  Validar prova de continuidade
3.  Publicar `resurrection_report.json`
4.  Atualizar `vault/history` com **Generation+1**

* * *

ğŸ”§ Tarefas por PR
-----------------

**PR M1 â€” Ritual de Desligamento**

*   `m1_shutdown_ritual.sh`, `make_last_manifest.sh`, `capture_telemetry.sh`
*   `logline sign/verify` integrado
*   Testes: bancos Ã­ntegros, hash recursive, assinatura OK

**PR M2 â€” HibernaÃ§Ã£o & GuardiÃ£o**

*   `sleepguardd` (Rust), `lockdown.sh`, `resume.sh`
*   Flags imutÃ¡veis, sampling de verificaÃ§Ã£o, relatÃ³rios jsonl
*   Teste: 24h sem divergÃªncia e sem alarme falso

**PR M3 â€” RessurreiÃ§Ã£o**

*   `logline revive` (wrapper), `resurrection_playbook.md`
*   Prova de continuidade (frame match / fingerprint)
*   Teste: revival em nÃ³ alternativo (RTO < 30 min)

* * *

ğŸ§© IntegraÃ§Ã£o com o resto do sistema
------------------------------------

*   O **Epic M** conversa com:
    *   **Epic A/B** (setup base, configs, CLI)
    *   **Epic F** (queue/broadcast/watchdogs)
    *   **Epic G** (QC para prova de continuidade)
*   As chaves/assinaturas e manifestos ficam centralizados em `/vvtv/vault/â€¦`

* * *

âœ… Resumo
--------

*   **Desligar** com snapshot **assinado**, manifesto **verificÃ¡vel** e **frame final** arquivado.
*   **Hibernar** com **imutabilidade** e guardiÃ£o ativo **sem escrever** nos dados.
*   **Ressuscitar** qualquer nÃ³ a partir de snapshots, **provando continuidade** e **sem perder estado**.

Se quiser, eu jÃ¡ te entrego os **scripts M1/M2/M3** prontos (com caminhos do teu ambiente) e um â€œmini-snapshotâ€ de teste para ensaio de revival em 15 minutos.

## Prompt:
Prefiro que faca o epico N desse jeito que fez o M

## Response:
Pensou por 9s

perfeito â€” segue o **Epic N** no mesmo formato do Epic M: documento Ãºnico, â€œexecutÃ¡velâ€, pronto para entrar no repo como `docs/epics/Epic-N_Public_API_MultiTenant.md`.

* * *

ğŸŒ Epic N â€” Public API & Multi-Tenant LLM Pool (Blueprint ExecutÃ¡vel)
=====================================================================

> Objetivo: expor o **LLM Pool** na rede de forma **segura, multi-tenant, com chaves**, **rate limit**, **quotas**, **observabilidade**, **SLOs** e **PRs automÃ¡ticos** para governanÃ§a. Pronto para uso interno e parceiros.

**Ã‚mbito do Epic**

*   **N1 â€” Gateway & Auth (HMAC/JWT/mTLS)**
*   **N2 â€” Tenants, Quotas & Rate-Limit**
*   **N3 â€” Auditoria, MÃ©tricas & Faturamento (usage)**
*   **N4 â€” RotaÃ§Ã£o de Segredos & Onboarding Automatizado**
*   **N5 â€” Edge/DMZ Deployment & WAF**
*   **N6 â€” SLOs, Testes de Carga & Chaos Scenarios**

* * *

ğŸ“¦ EntregÃ¡veis (artefatos e serviÃ§os)
-------------------------------------

*   **Gateway**: NGINX/Envoy com rotas `/v1/infer`, `/v1/health`, `/v1/usage`
*   **Auth**: HMAC **e** JWT opcional (com `kid`/JWKS), suporte mTLS
*   **Tenant DB**: `tenants.sqlite` (chaves, quotas, planos, escopos)
*   **Rate-limit**: token bucket por tenant + burst, por rota
*   **Auditoria**: `audit/api_requests.jsonl` (append-only, com `request_id`)
*   **MÃ©tricas**: Prometheus + Grafana dashboards â€œAPI Healthâ€ e â€œTenant Usageâ€
*   **Faturamento**: `usage_rollup_daily.json` (tokens, requests, bytes, latÃªncia)
*   **Runbooks**: onboarding, rotaÃ§Ã£o de chaves, incidente de abuso
*   **Infra**: manifests (launchd/systemd), `nginx.conf`, `envoy.yaml`
*   **Testes**: carga (k6), latÃªncia, rate-limit, auth, caos (falha de provider)

* * *

ğŸ—‚ï¸ Estrutura de diretÃ³rios
---------------------------

```
/vvtv/
â”œâ”€ api/
â”‚  â”œâ”€ gateway/               # nginx/ ou envoy/
â”‚  â”‚  â”œâ”€ nginx.conf
â”‚  â”‚  â””â”€ envoy.yaml
â”‚  â”œâ”€ configs/
â”‚  â”‚  â”œâ”€ api.toml            # portas, thresholds, SLOs
â”‚  â”‚  â””â”€ jwks.json           # opcional (se JWT externo)
â”‚  â”œâ”€ bin/
â”‚  â”‚  â”œâ”€ start_gateway.sh
â”‚  â”‚  â”œâ”€ stop_gateway.sh
â”‚  â”‚  â”œâ”€ rotate_keys.sh
â”‚  â”‚  â””â”€ smoke.sh
â”‚  â””â”€ schemas/               # JSON Schemas de requests/responses
â”œâ”€ pool/                      # serviÃ§o LLM Pool jÃ¡ existente
â”œâ”€ data/
â”‚  â””â”€ tenants.sqlite         # tenants, keys, quotas, planos
â”œâ”€ logs/
â”‚  â”œâ”€ api_access.log
â”‚  â”œâ”€ api_error.log
â”‚  â””â”€ audit/api_requests.jsonl
â””â”€ dashboards/
   â”œâ”€ api_health.json        # Grafana
   â””â”€ tenant_usage.json
```

* * *

âœ… N1 â€” Gateway & Auth
---------------------

### Rotas pÃºblicas

*   `GET /v1/health/ready` â†’ 200 se **â‰¥1 provider saudÃ¡vel** e pool pronto
*   `POST /v1/infer` â†’ inferÃªncia (task/strategy + payload padronizado)
*   `GET /v1/usage?from=â€¦&to=â€¦` â†’ mÃ©tricas do tenant autenticado (resumo)
*   `POST /v1/keys/rotate` â†’ (opcional) auto-rotaÃ§Ã£o com polÃ­tica e webhooks

### HMAC (simples e robusto)

**CabeÃ§alhos requeridos:**

*   `X-Api-Key: <public_key>`
*   `X-Timestamp: <epoch_ms>`
*   `X-Nonce: <uuid4>`
*   `X-Signature: base64( HMAC_SHA256( secret, method|path|body|ts|nonce ) )`

**ValidaÃ§Ãµes:**

*   Chave existe e ativa no `tenants.sqlite`
*   `|now - ts| <= 5 min`
*   Nonce sem replay (cache em memÃ³ria/redis 10 min)
*   Assinatura confere

**Resposta em falha:** `401/403` com `error: "AUTH_FAILED"` ou `409 NONCE_REPLAY`.

### JWT (opcional)

*   `Authorization: Bearer <jwt>`
*   ValidaÃ§Ã£o de `kid` em `jwks.json` ou endpoint externo
*   Claims mÃ­nimas: `tenant_id`, `scopes` (ex.: `["infer"]`), `exp <= 15m`
*   Pode coexistir com HMAC (modo â€œqualquer um que passarâ€)

### mTLS (opcional, partners premium)

*   VerificaÃ§Ã£o de certificado do cliente no gateway (CA interna)
*   Mapeia `CN` â†’ `tenant_id` (tabela de mapeamento)

### Exemplo NGINX (nÃºcleo das rotas)

```nginx
server {
  listen 7071;
  server_name api.vvtv.local;

  # health (sem auth)
  location = /v1/health/ready { proxy_pass http://127.0.0.1:7070/health/ready; }

  # infer (com rate-limit por tenant/ip)
  location = /v1/infer {
    proxy_pass http://127.0.0.1:7070/infer;
    proxy_read_timeout 120;
    proxy_connect_timeout 2;
    limit_req zone=perip burst=20 nodelay;
    # headers repassados (auth verificada no app/pool-api)
    proxy_set_header X-Api-Key   $http_x_api_key;
    proxy_set_header X-Timestamp $http_x_timestamp;
    proxy_set_header X-Nonce     $http_x_nonce;
    proxy_set_header X-Signature $http_x_signature;
  }

  # usage (autenticado, leitura)
  location = /v1/usage {
    proxy_pass http://127.0.0.1:7070/usage;
    proxy_read_timeout 10;
    proxy_connect_timeout 2;
  }
}
```

> **Nota:** a validaÃ§Ã£o **criptogrÃ¡fica** (HMAC/JWT), identificaÃ§Ã£o de tenant e rate-limit **por tenant** sÃ£o feitas no **pool-api** (camada app atrÃ¡s do gateway), nÃ£o apenas por IP.

* * *

ğŸ“Š N2 â€” Tenants, Quotas & Rate-Limit
------------------------------------

### Esquema `tenants.sqlite`

```
tenants(
  id TEXT PK,
  name TEXT,
  status TEXT CHECK(status IN ('active','suspended')),
  plan TEXT,                          -- free, pro, enterprise
  api_public_key TEXT UNIQUE,
  api_secret_hash TEXT,               -- scrypt/argon2id
  rpm_limit INTEGER,                  -- requests/min
  rps_burst INTEGER,                  -- burst
  tpm_limit INTEGER,                  -- tokens/min (opcional)
  scopes TEXT,                        -- json array
  created_at, updated_at
)

nonces( tenant_id TEXT, nonce TEXT, ts INTEGER, PRIMARY KEY(tenant_id, nonce) )
usage_events( ts INTEGER, tenant_id TEXT, route TEXT, tokens_in INTEGER, tokens_out INTEGER, duration_ms INTEGER, status INTEGER, request_id TEXT )
```

### Rate-limit (token bucket)

*   **Por tenant**: `rpm_limit` (minuto), `rps_burst` (explosÃ£o curta)
*   **Por rota**: pesos (ex.: `infer=1`, `usage=0.2`)
*   **Resposta**: `429` + `Retry-After: 1`

### Quotas mensais

*   Tabela de **rollup diÃ¡rio** â†’ soma mensal
*   Travas â€œsoftâ€ (warning 80/90%) e â€œhard stopâ€ (100%, retorna 402/429)

* * *

ğŸ§¾ N3 â€” Auditoria, MÃ©tricas & Faturamento
-----------------------------------------

### Auditoria (append-only)

Arquivo: `/vvtv/logs/audit/api_requests.jsonl`

Campos:

```json
{
  "ts":"2025-10-22T12:34:56Z",
  "request_id":"uuid",
  "tenant_id":"t_abc",
  "route":"/v1/infer",
  "task":"expand_queries",
  "strategy":"FASTEST",
  "status":200,
  "duration_ms":412,
  "tokens_in":102,
  "tokens_out":210,
  "provider":"phi3-mini",
  "from_cache":false,
  "hedge":false,
  "ip":"10.0.0.50"
}
```

### MÃ©tricas Prometheus

*   `api_http_duration_ms{route,tenant_id}` (histogram)
*   `api_requests_total{route,code}` (counter)
*   `api_rate_limited_total{tenant_id}` (counter)
*   `api_quota_exceeded_total{tenant_id}` (counter)
*   `llmpool_infer_duration_ms{provider,task}` (reuso das mÃ©tricas do pool)
*   `llmpool_timeouts_total{provider,task}`

**SLOs propostos (p95, janela 7d):**

*   `/v1/infer`: **â‰¤ 1200 ms** (apÃ³s warm)
*   Erro 5xx total **< 0.5%**
*   Disponibilidade **â‰¥ 99.5%**

### Faturamento (usage)

*   Job de **rollup diÃ¡rio**:
    *   `usage_rollup_daily.json` por tenant (tokens, requests, p95, bytes)
*   Export para CSV/Parquet (integraÃ§Ã£o contÃ¡bil)
*   Webhook opcional `POST /billing/usage`

* * *

ğŸ” N4 â€” RotaÃ§Ã£o de Segredos & Onboarding
----------------------------------------

### Onboarding de tenant (CLI)

```bash
vvtvctl tenants create --name "Acme" --plan pro --rpm 120 --burst 30 --scopes '["infer"]'
# retorna: tenant_id, api_public_key, api_secret (mostrar 1x)
```

*   Secret armazenado como **hash** (argon2id) â€” nunca em claro
*   Envio seguro ao cliente (canal out-of-band)

### RotaÃ§Ã£o (chaves HMAC)

*   `POST /v1/keys/rotate` (se habilitado e scope permitir)
*   PolÃ­tica: chave nova **ativa em T0**, chave antiga **expira em T0+7d**
*   Gate: rate-limit especÃ­fico para rotaÃ§Ã£o (1/dia)

### Chaves JWT (se usado)

*   `jwks.json` com `kid` e rotaÃ§Ã£o trimestral
*   TolerÃ¢ncia simultÃ¢nea de **2 chaves** por 7 dias

* * *

ğŸ›¡ï¸ N5 â€” Edge/DMZ & WAF
-----------------------

*   Gateway em **mÃ¡quina separada** (ou container) com:
    *   WAF bÃ¡sico (OWASP CRS se usar NGINX/ModSecurity)
    *   **mTLS** opcional p/ parceiros enterprise
*   Pool fica **em rede interna**; somente API do gateway chega nele
*   **mTLS interno** opcional entre gateway â†” pool

**Headers de seguranÃ§a:**

*   `X-Request-Id` gerado no gateway
*   Limitar tamanhos: `client_max_body_size 1m` (JSON curto)
*   `Content-Security-Policy` irrelevante (API), mas defina `X-Content-Type-Options: nosniff`

* * *

ğŸ§ª N6 â€” SLOs, Carga & Chaos
---------------------------

### Testes de carga (k6, exemplos)

*   50 rps / 2 min / payload pequeno (`expand_queries`)
*   10 rps / 2 min / payload grande (`judge/rerank` 10â€“12 itens)
*   Ver:
    *   p50/p95, erro 5xx, timeouts
    *   hit de cache no pool

### Chaos (injetar falhas)

*   Simular **falha de provider** (breaker abre) â†’ hedge assume
*   Induzir **latÃªncia** de rede no Ollama â†’ p95 ainda â‰¤ SLO?
*   Falhar **Auth** (assinatura invÃ¡lida) â†’ 403 rÃ¡pido (â‰¤5ms de CPU)

### Sinais de â€œgo/no-goâ€

*   p95 â‰¤ 1.2s em `/v1/infer` com 10 rps sustentados
*   5xx < 0.5% por 30 min
*   rate-limit dispara < 2% com clientes â€œbem-educadosâ€

* * *

ğŸ”Œ Contratos de API
-------------------

### `POST /v1/infer` (body)

```json
{
  "task": "expand_queries | judge | rerank_candidates | enrich_metadata | recovery_plan",
  "strategy": "FASTEST | VOTING | WEIGHTED | JUDGE", 
  "payload": { "...dados especÃ­ficos da tarefa..." },
  "hints": { "deadline_ms": 1200, "max_tokens": 256 }
}
```

**Resposta padrÃ£o**

```json
{
  "request_id": "uuid",
  "tenant_id": "t_abc",
  "winner_model": "phi3:mini",
  "strategy_used": "FASTEST",
  "duration_ms": 412,
  "from_cache": false,
  "content": { "...schema da tarefa..." }
}
```

**Erros**

*   `401/403 AUTH_FAILED`
*   `409 NONCE_REPLAY`
*   `429 RATE_LIMITED` (Retry-After)
*   `422 SCHEMA_INVALID`
*   `502 PROVIDER_FAILED`
*   `504 DEADLINE_EXCEEDED`

* * *

ğŸ§° Exemplos prÃ¡ticos
--------------------

### `curl` HMAC (cliente)

```bash
BODY='{"task":"expand_queries","strategy":"FASTEST","payload":{"seed":"abc"}}'
TS=$(date +%s%3N); NONCE=$(uuidgen); KEY="vvtv-pub-001"; SECRET="â€¦"

SIG=$(printf "POST|/v1/infer|%s|%s|%s" "$BODY" "$TS" "$NONCE" \
  | openssl dgst -sha256 -hmac "$SECRET" -binary | base64)

curl -s http://api.vvtv.local:7071/v1/infer \
  -H "Content-Type: application/json" \
  -H "X-Api-Key: $KEY" \
  -H "X-Timestamp: $TS" \
  -H "X-Nonce: $NONCE" \
  -H "X-Signature: $SIG" \
  -d "$BODY" | jq .
```

### Resposta esperada

```json
{
  "request_id": "f527â€¦",
  "tenant_id": "t_abc",
  "winner_model": "phi3:mini",
  "strategy_used": "FASTEST",
  "duration_ms": 318,
  "from_cache": true,
  "content": {
    "must_include": ["â€¦"],
    "optional": ["â€¦"],
    "negatives": [],
    "duration_range_sec": [540,720],
    "site_hints": ["â€¦"]
  }
}
```

* * *

ğŸ”­ Observabilidade & Alertas
----------------------------

### Dashboards

*   **API Health**: `api_http_duration_ms`, `api_requests_total`, `api_rate_limited_total`, `5xx_rate`
*   **Tenant Usage**: `requests`, `tokens_in/out`, `latÃªncia p95`, `erro por tenant`

### Alertas (Prometheus)

*   `5xx_rate > 0.005` (5m) â†’ **warning**
*   `p95_infer > 1500ms` (10m) â†’ **warning**
*   `quota_exceeded_total{tenant} > 0` â†’ **info** + webhook
*   `breaker_open_total{provider} > 0` (5m) â†’ **warning** (ver provider)

* * *

ğŸ“˜ Runbooks
-----------

### Onboarding

1.  `vvtvctl tenants create â€¦`
2.  Enviar `api_public_key` + instruÃ§Ãµes HMAC
3.  Teste `curl` do cliente (sandbox)
4.  Colocar em **pro** (se aplicÃ¡vel), setar quotas

### RotaÃ§Ã£o de chave

1.  Rodar `rotate_keys.sh` (ou `POST /v1/keys/rotate`)
2.  Informar cliente com **T0/T0+7d**
3.  Monitorar falhas 403

### Incidente de abuso (DoS lÃ³gico)

1.  `suspend tenant` (status=`suspended`)
2.  Aumentar `hedge_after_ms` temporÃ¡rio
3.  Ajustar `burst` no gateway
4.  Post-mortem no `audit/` + recomendaÃ§Ã£o

* * *

ğŸ§ª Testes e AceitaÃ§Ã£o
---------------------

### Unit/Integration

*   HMAC/JWT vÃ¡lidos e invÃ¡lidos
*   Nonce replay
*   Rate-limit por tenant (rps/rpm)
*   Quota mensal (hard/soft)
*   Esquemas por tarefa (422 se invÃ¡lido)
*   SanitizaÃ§Ã£o estrita de JSON (saÃ­da do pool)

### Carga

*   `k6` com 10â€“50 rps (curto) e 5â€“10 rps (longo)
*   Ver p95, 5xx, hedge, breaker

### Chaos

*   Matar provider (Ollama) â†’ 502 limitados, breaker abre, hedge assume
*   Atraso artificial â†’ deadline respeitado (504 controlado)

**CritÃ©rios de aceitaÃ§Ã£o**

*   p95 â‰¤ 1.2s estÃ¡vel
*   5xx < 0.5%
*   Rate-limit/quota atuando corretamente
*   Logs/auditoria consistentes e sem PII sensÃ­vel
*   Dashboards completos + alertas disparam conforme esperado

* * *

ğŸ”§ Tarefas por PR
-----------------

**PR N1 â€” Gateway & Auth**

*   `nginx.conf`/`envoy.yaml`, `start_gateway.sh`, `smoke.sh`
*   Validador HMAC/JWT no **pool-api**
*   Testes de unidade de assinatura

**PR N2 â€” Tenants & Limits**

*   `tenants.sqlite` schema + migraÃ§Ãµes
*   Middleware `tenant_resolver` + nonces
*   Token bucket por tenant (rps) + quotas mensais (rpm/tokens)

**PR N3 â€” Auditoria & Usage**

*   `audit/api_requests.jsonl` (append-only)
*   Export diÃ¡rio `usage_rollup_daily.json` + webhook
*   Dashboards Grafana (API Health, Tenant Usage)

**PR N4 â€” Rotation & Onboarding**

*   `vvtvctl tenants create/rotate/suspend`
*   PolÃ­ticas T0/T0+7d
*   DocumentaÃ§Ã£o de cliente (HMAC sample)

**PR N5 â€” Edge/DMZ**

*   WAF bÃ¡sico + limites de corpo
*   mTLS opcional (parceiros)
*   Hardening de SO/Firewall

**PR N6 â€” SLOs & Chaos**

*   Testes k6 (scripts em `api/tests/k6/`)
*   Fault-injection (provider down, latency)
*   Alertas Prometheus

* * *

âœ… Resumo
--------

*   **API pÃºblica segura** com HMAC/JWT/mTLS
*   **Multi-tenant** com rate-limit/quotas e auditoria
*   **MÃ©tricas & SLOs** prontos para produÃ§Ã£o
*   **GovernanÃ§a** com rotaÃ§Ã£o de chaves e PRs automÃ¡ticos (usage/report)
*   **ResiliÃªncia**: hedge, breaker, deadlines, caos testado

> **Mantra:** _expor com elegÃ¢ncia, limitar com justiÃ§a, medir com obsessÃ£o._

Se quiser, te entrego os **templates reais** (`nginx.conf`, `envoy.yaml`, `api.toml`, `jwks.json`, `k6 scripts`) jÃ¡ preenchidos para o teu Mac mini e rede local â€” Ã© sÃ³ colar e `./bin/start_gateway.sh`.

## Prompt:
Agora o O

## Response:
perfeito â€” segue o **Epic O** no mesmo formato dos Epics M e N: documento Ãºnico, executÃ¡vel, para entrar no repo como `docs/epics/Epic-O_Operations_OnCall_SRE.md`.

* * *

ğŸ› ï¸ Epic O â€” Operations, On-Call & SRE Governance (Blueprint ExecutÃ¡vel)
========================================================================

> Objetivo: padronizar **operaÃ§Ãµes, plantÃ£o e confiabilidade** do VVTV (streaming + business logic + LLM Pool), com **SLOs claros**, **alertas Ãºteis**, **runbooks clicÃ¡veis**, **rotina de mudanÃ§as segura** e **resposta a incidentes** com pÃ³s-mortem.

**Ã‚mbito**

*   **O1 â€” SLOs, SLIs & OrÃ§amentos de erro**
*   **O2 â€” Observabilidade unificada (logs, mÃ©tricas, traÃ§os)**
*   **O3 â€” Alertas que importam e nÃ£o acordam Ã  toa**
*   **O4 â€” On-call: escalas, ferramentas, runbooks operacionais**
*   **O5 â€” GestÃ£o de mudanÃ§as (change mgmt) & feature flags**
*   **O6 â€” Capacidade, disponibilidade, DR & exercÃ­cios de caos**

* * *

ğŸ“¦ EntregÃ¡veis
--------------

*   **CatÃ¡logo de SLOs** por serviÃ§o (stream, planner, pool, API pÃºblica)
*   **Dashboards Grafana**: â€œOps Overviewâ€, â€œStreaming Healthâ€, â€œLLM Poolâ€, â€œAPI Healthâ€
*   **Alertas Prometheus** com rÃ³tulos por severidade/rota
*   **Runbooks** shell/markdown em `/vvtv/ops/runbooks/â€¦`
*   **Playbooks de incidente** com _checklists_ e _timers_
*   **Ferramentas**: `vvtvctl ops` (status, drains, toggles), _feature flags_ TOML
*   **CalendÃ¡rio de On-Call** (iCal) + Escalonamento (pessoa â†’ coordenaÃ§Ã£o)
*   **MudanÃ§as**: `change_requests/CR-*.md` + _preflight_ automatizado
*   **PÃ³s-mortem**: `postmortems/PM-*.md` com liÃ§Ãµes e aÃ§Ãµes rastreÃ¡veis

* * *

ğŸ—‚ï¸ Estrutura de diretÃ³rios
---------------------------

```
/vvtv/
â”œâ”€ ops/
â”‚  â”œâ”€ runbooks/
â”‚  â”‚  â”œâ”€ rb_streaming.md              # NGINX-RTMP/FFmpeg
â”‚  â”‚  â”œâ”€ rb_pool.md                   # LLM Pool
â”‚  â”‚  â”œâ”€ rb_api.md                    # Public API
â”‚  â”‚  â”œâ”€ rb_planner.md                # Planner/Queue/Curator
â”‚  â”‚  â”œâ”€ rb_db.md                     # SQLite (WAL, backup, vacuum)
â”‚  â”‚  â”œâ”€ rb_security.md               # chaves, rotaÃ§Ã£o, bloqueios
â”‚  â”‚  â””â”€ rb_discovery.md              # Search/Download/Browser sim
â”‚  â”œâ”€ playbooks/
â”‚  â”‚  â”œâ”€ pb_incident_major.md
â”‚  â”‚  â”œâ”€ pb_cs_intrusion.md
â”‚  â”‚  â””â”€ pb_dr_failover.md
â”‚  â”œâ”€ slo/
â”‚  â”‚  â”œâ”€ slo_streaming.yaml
â”‚  â”‚  â”œâ”€ slo_pool.yaml
â”‚  â”‚  â”œâ”€ slo_api.yaml
â”‚  â”‚  â””â”€ slo_planner.yaml
â”‚  â”œâ”€ change_requests/
â”‚  â”‚  â””â”€ CR-YYYYMMDD-<slug>.md
â”‚  â”œâ”€ postmortems/
â”‚  â”‚  â””â”€ PM-YYYYMMDD-<slug>.md
â”‚  â”œâ”€ oncall/
â”‚  â”‚  â”œâ”€ rota.ics
â”‚  â”‚  â””â”€ escalation.yaml
â”‚  â””â”€ flags/
â”‚     â””â”€ features.toml                # feature flags
â”œâ”€ dashboards/
â”‚  â”œâ”€ ops_overview.json
â”‚  â”œâ”€ streaming_health.json
â”‚  â”œâ”€ llm_pool.json
â”‚  â””â”€ api_health.json
â””â”€ alerts/
   â””â”€ prometheus_rules.yaml
```

* * *

âœ… O1 â€” SLOs, SLIs & OrÃ§amento de erro
-------------------------------------

### Tabela de SLOs (p95, janela 7 dias)

| ServiÃ§o | SLI | Alvo SLO | OrÃ§amento de erro |
| --- | --- | --- | --- |
| **Streaming** | `segment_age_seconds` | â‰¤ **6s** (HLS LL) | 0.5% janelas |
|  | `encoder_drops_rate` | < **0.1%** | 0.2% |
| **LLM Pool** | `/infer p95` | â‰¤ **1200ms** warm | 1% |
|  | `5xx rate` | < **0.5%** | 0.5% |
| **API PÃºblica** | `/v1/infer` p95 | â‰¤ **1200ms** | 1% |
|  | `5xx rate` | < **0.5%** | 0.5% |
| **Planner** | `selection_cycle_duration_ms p95` | â‰¤ **500ms** | 1% |
|  | `queue_starvation` (buffer < 30min) | **0** eventos/7d | zero tolerance |

**DefiniÃ§Ãµes rÃ¡pidas**

*   `segment_age_seconds`: idade do Ãºltimo segmento ao vivo.
*   `queue_starvation`: buffer < 30 min por â‰¥ 2 min.

Arquivos YAML por serviÃ§o (ex.: `ops/slo/slo_streaming.yaml`) definem:

*   **MÃ©trica**, **alvo**, **janela**, **owner**, **painel grafana**, **alertas vinculados**.

* * *

ğŸ“ˆ O2 â€” Observabilidade Unificada
---------------------------------

### Logs estruturados (JSONL)

*   `logs/streaming.jsonl` (ingest, ffmpeg, segmenter)
*   `logs/pool.jsonl` (task, strategy, provider, duration, cache/hedge)
*   `logs/api_access.log` (como N mas com `request_id`)
*   `logs/curator_vigilante.jsonl`, `logs/autopilot.jsonl`

**Campo padrÃ£o**: `ts`, `service`, `severity`, `msg`, `trace_id`, `span_id`

### MÃ©tricas Prometheus (exemplos)

*   `stream_segment_age_seconds`
*   `stream_encoder_drops_total`
*   `llmpool_infer_duration_ms` (histogram)
*   `llmpool_timeouts_total`
*   `api_http_duration_ms{route}`
*   `api_requests_total{code}`
*   `planner_cycle_duration_ms`
*   `queue_buffer_duration_minutes`

### TraÃ§os (opcional, OpenTelemetry)

*   `trace_id` passado da **API â†’ Pool â†’ Provider**
*   Amostragem 1â€“5% para baixa sobrecarga

### Dashboards Grafana

*   **Ops Overview**: visÃ£o executiva (SLOs, erros, backlog, buffer)
*   **Streaming Health**: segment\_age, fps, drops, ingest bitrate
*   **LLM Pool**: p95 por task/provider, cache hit, breaker, hedge rate
*   **API Health**: 5xx, p95 por rota, rate-limit/quota

* * *

ğŸ”” O3 â€” Alertas Ãºteis (sem pager fatigue)
-----------------------------------------

`alerts/prometheus_rules.yaml` (resumo):

*   **CRÃTICO**
    *   `stream_segment_age_seconds > 10` por 2 min
    *   `queue_starvation` detectado
    *   `llmpool_timeouts_total` aumenta > 10/min por 5 min
    *   `api_5xx_rate > 1%` por 5 min
*   **MAJOR**
    *   `p95_infer > 1500ms` por 10 min
    *   `encoder_drops_rate > 0.2%` por 10 min
    *   `buffer_duration_minutes < 45` por 10 min
*   **MINOR**
    *   `curator_apply_rate > 20%` por 1h (config suspeita)
    *   `autopilot_rollbacks_total > 0` (investigar em horÃ¡rio comercial)

**Etiqueta de alerta**: `service`, `severity`, `runbook_url`  
Cada alerta tem `annotations.runbook_url` apontando para um _anchor_ no runbook.

* * *

ğŸ§‘â€ğŸš’ O4 â€” On-Call, Escalonamento & Runbooks
--------------------------------------------

### Rota On-Call

*   Arquivo: `ops/oncall/rota.ics` (turnos semanais)
*   Escalonamento: `ops/oncall/escalation.yaml`
    *   NÃ­vel 1: Operador (24/7)
    *   NÃ­vel 2: SRE LÃ­der (08:00â€“22:00)
    *   NÃ­vel 3: Eng Prod (em horÃ¡rio comercial)
    *   _Timeouts_: 5m â†’ 10m â†’ 20m

### Ferramentas On-Call

*   `vvtvctl ops status` (saÃºde consolidada)
*   `vvtvctl ops drain` (colocar serviÃ§o em manutenÃ§Ã£o)
*   `vvtvctl flags set <flag>=on|off` (feature flags)
*   `vvtvctl snapshot quick` (snapshot leve de diagnÃ³stico)

### Runbooks (exemplos prÃ¡ticos)

#### `rb_streaming.md` (trecho)

*   Sintoma: **segment\_age > 10s**
*   AÃ§Ãµes (em ordem):
    1.  `vvtvctl streaming status`
    2.  Ver ingest bitrate (`ffprobe`), CPU, IO
    3.  **Se encoder travado**: `vvtvctl streaming restart-encoder` (sem derrubar NGINX)
    4.  **Se buffer < 30min**: injetar **emergency loop**
    5.  Capturar `ops bundles` (`vvtvctl ops bundle --scope streaming`)

**Timer**: 10 min para estabilizar **antes** de escalar nÃ­vel 2.

#### `rb_pool.md`

*   Sintoma: **p95\_infer > 1500ms**
*   AÃ§Ãµes:
    1.  `vvtvctl pool health`
    2.  Ver **breaker**/hedge rate
    3.  Desabilitar provider com pior latÃªncia: `vvtvctl pool provider disable <id>`
    4.  Reduzir `deadline_ms` de tarefa pesada via **flag** temporÃ¡ria
    5.  Se com API pÃºblica: ativar **burst shedding** (limitar `/v1/infer` a rps 60 por tenant)

#### `rb_api.md`

*   Sintoma: **5xx\_rate > 1%**
*   AÃ§Ãµes:
    1.  Checar autenticaÃ§Ã£o (replay/nonce)
    2.  Aplicar **ban temporÃ¡rio** ao tenant abusivo
    3.  Ver conns no gateway, _backpressure_ para `/infer`

> Cada runbook tem **comandos copy-paste** e **checklist** (box de ticar).

* * *

ğŸ§® O5 â€” MudanÃ§as, _Feature Flags_ & Janela de ManutenÃ§Ã£o
--------------------------------------------------------

### Change Requests (CR)

Modelo: `ops/change_requests/CR-YYYYMMDD-<slug>.md` com:

*   **DescriÃ§Ã£o**, **riscos**, **roll-back**, **checklist de prÃ©-voo**
*   **Janela** (ex.: Domingo 02:00â€“03:00 UTC)
*   **Owner** e **AprovaÃ§Ã£o**

**Preflight automÃ¡tico** (CI ou script):

*   Teste de carga leve (k6 smoke)
*   `vvtvctl validate all`
*   Dry-run de configuraÃ§Ãµes (YAML/TOML)

### Feature Flags

Arquivo: `ops/flags/features.toml`

```toml
[curator]
apply_enabled = true
min_confidence_apply = 0.62

[autopilot]
enabled = false
canary_pct = 0.2
```

CLI:

```bash
vvtvctl flags get curator.apply_enabled
vvtvctl flags set curator.apply_enabled=false
```

### Janelas

*   **ManutenÃ§Ã£o padrÃ£o**: Domingos 02:00â€“03:00 UTC
*   **Congelamento**: vÃ©spera de eventos (ex.: promoÃ§Ãµes) â†’ **sem mudanÃ§as**

* * *

ğŸ“¦ O6 â€” Capacidade, DR & Caos
-----------------------------

### Planejamento de Capacidade

*   Painel â€œCapacity Forecastâ€:
    *   CPU mÃ©dia e p95 por pipeline (encoder, pool, API)
    *   RAM livre, _swap_ zero (meta)
    *   Disco: alerta em 80%, _action_ em 90%
*   PolÃ­tica:
    *   Adicionar instÃ¢ncia de encoder se `encoder_drops_rate` toca 0.1% por 3 dias
    *   Aumentar keep-alive do modelo (Ollama) em janelas de pico

### DR/Backups

*   Banco SQLite: **WAL + snapshots diÃ¡rios** (integra com Epic M)
*   Configs & flags: backup versionado (git) + tar.zst diÃ¡rio
*   Teste DR trimestral (revival em nÃ³ reserva) â€” usar playbook do **Epic M**

### Chaos Engineering

*   CenÃ¡rios mensais:
    *   **Kill provider** (ver hedge/breaker e p95)
    *   **IO throttling** em storage (stream deve seguir com emergency loop)
    *   **LatÃªncia artificial** no gateway (p95 API < 1.5s?)

* * *

ğŸ§° Exemplos de comandos operacionais
------------------------------------

```bash
# SaÃºde consolidada
vvtvctl ops status

# Drenar pool (manutenÃ§Ã£o sem derrubar API)
vvtvctl ops drain --service pool --reason "upgrade provider weights"

# Desligar provider com latÃªncia anÃ´mala
vvtvctl pool provider disable ollama-gemma-9b

# Habilitar emergÃªncia streaming
vvtvctl streaming emergency --inject 5 --reason "buffer<30m"

# Congelar curador (modo advice-only)
vvtvctl flags set curator.apply_enabled=false
```

* * *

ğŸ§ª Testes & AceitaÃ§Ã£o
---------------------

### Unit/Integration

*   Export de mÃ©tricas (todos os serviÃ§os respondem)
*   Alertas disparam em _fixtures_ de teste (simulaÃ§Ãµes)
*   Runbooks referenciados por todos os alertas (link OK)
*   Flags aplicam efeitos sem restart

### Carga

*   _Soak test_ de 2h (5â€“10 rps contÃ­nuos na API pÃºblica)
*   p95 dentro do SLO, 5xx < 0.5%

### Caos

*   Provider down â†’ breaker abre, hedge assume, SLO mantido
*   Encoder reiniciado â†’ segment\_age nÃ£o ultrapassa 10s por >2min

**CritÃ©rios de aceitaÃ§Ã£o**

*   Dashboards com **todas** as mÃ©tricas chave
*   Alertas sem falsos positivos por 7 dias
*   On-call com playbooks testados em _game day_ trimestral
*   CR de mudanÃ§a com _preflight_ automatizado

* * *

ğŸ§­ Playbook de Incidente (resumo)
---------------------------------

1.  **Reconhecer** alerta (pager)
2.  **Nomear IMOC** (Incident Manager On Call)
3.  **Abrir canal** `#incident-YYYYMMDD-HHMM`
4.  **Status page** interna (severidade + ETA)
5.  **Mitigar** com runbook (10 min alvo para MAJOR)
6.  **Registrar** linha do tempo (timestamps)
7.  **Encerrar** com comunicaÃ§Ã£o e **aÃ§Ã£o corretiva**
8.  **PÃ³s-mortem** em 48h: causas, aÃ§Ãµes, owners, prazos

* * *

ğŸ”§ Tarefas por PR
-----------------

**PR O1 â€” SLOs & PainÃ©is**

*   Definir SLO YAML por serviÃ§o
*   Criar dashboards Grafana (Ops Overview, etc.)
*   Expor mÃ©tricas que faltavam

**PR O2 â€” Alertas & Runbooks**

*   Escrever `alerts/prometheus_rules.yaml`
*   Criar runbooks em `ops/runbooks/*` com _anchors_ (links de alerta)
*   Validar semÃ¢ntica + _dry-run_ de regras

**PR O3 â€” On-Call & Escalonamento**

*   `oncall/rota.ics` e `escalation.yaml`
*   Template de _status update_ e _war room_

**PR O4 â€” vvtvctl ops & Flags**

*   Comandos `ops status|drain|bundle`
*   _Feature flags_ TOML + CLI `flags`

**PR O5 â€” MudanÃ§as & Preflight**

*   Template CR e _preflight_ (validate + k6 smoke)
*   Make targets `make preflight`, `make deploy-change CR=â€¦`

**PR O6 â€” DR & Caos**

*   Scripts de caos (latÃªncia, kill provider/encoder)
*   Procedimentos de revival (integra Epic M)
*   Game-day trimestral documentado

* * *

âœ… Resumo
--------

*   **SLOs claros** + **alertas acionÃ¡veis** + **runbooks executÃ¡veis**
*   **On-call leve** (menos barulho, mais contexto, mais automaÃ§Ã£o)
*   **MudanÃ§as seguras** com _feature flags_ e _preflight_
*   **ResiliÃªncia real** com DR testado e exercÃ­cios de caos
*   **OperaÃ§Ã£o auditÃ¡vel**: o que fizemos, quando e por quÃª â€” sempre documentado

> _â€œConfiabilidade nÃ£o Ã© ausÃªncia de falhas; Ã© capacidade de agir rÃ¡pido com boa informaÃ§Ã£o.â€_